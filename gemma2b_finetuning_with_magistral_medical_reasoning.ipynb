{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c619de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -Uq bitsandbytes==0.46.0\n",
    "!pip install -Uq peft==0.15.2\n",
    "!pip install -Uq trl==0.18.2\n",
    "!pip install -Uq accelerate==1.7.0\n",
    "!pip install -Uq datasets==3.6.0\n",
    "!pip install -Uq transformers==4.52.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773c75b2-2fbf-48e5-8e90-747b4b18ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wx-zh\\anaconda3\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, GemmaTokenizer\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5eb0ea-1582-450f-a263-ec7921f41ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8306b1d7-933e-48be-92c6-39e4ce66d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffde85df-257f-43a1-a42f-313f8b665470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n",
      "<transformers.integrations.tensor_parallel.ParallelInterface object at 0x00000235CE683C10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map={\"\":0},\n",
    "                                             token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348fc862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 3702\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mamachang/medical-reasoning\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda7810c-9217-40ec-83d3-e1156b9afebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 3702\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24737b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q:An 8-year-old boy is brought to the pediatri...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a clinical vignette desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q:A 23-year-old man comes to the physician bec...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a clinical vignette desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q:A 27-year-old man presents to the emergency ...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a question about a 27-ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q:A 13-year-old girl presents with a 4-week hi...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a patient with signs and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q:A 53-year-old Asian woman comes to the physi...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a patient with symptoms ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q:A 7-year-old boy is brought to the physician...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a clinical vignette desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q:A 21-year-old man comes to the military base...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a clinical case question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q:A 48-year-old woman presents to her primary ...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a question about determi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q:A 62-year-old man presents to the emergency ...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a patient with a history...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q:A 34-year-old female presents to her primary...</td>\n",
       "      <td>Please answer with one of the option in the br...</td>\n",
       "      <td>&lt;analysis&gt;\\n\\nThis is a clinical vignette desc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Q:An 8-year-old boy is brought to the pediatri...   \n",
       "1  Q:A 23-year-old man comes to the physician bec...   \n",
       "2  Q:A 27-year-old man presents to the emergency ...   \n",
       "3  Q:A 13-year-old girl presents with a 4-week hi...   \n",
       "4  Q:A 53-year-old Asian woman comes to the physi...   \n",
       "5  Q:A 7-year-old boy is brought to the physician...   \n",
       "6  Q:A 21-year-old man comes to the military base...   \n",
       "7  Q:A 48-year-old woman presents to her primary ...   \n",
       "8  Q:A 62-year-old man presents to the emergency ...   \n",
       "9  Q:A 34-year-old female presents to her primary...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Please answer with one of the option in the br...   \n",
       "1  Please answer with one of the option in the br...   \n",
       "2  Please answer with one of the option in the br...   \n",
       "3  Please answer with one of the option in the br...   \n",
       "4  Please answer with one of the option in the br...   \n",
       "5  Please answer with one of the option in the br...   \n",
       "6  Please answer with one of the option in the br...   \n",
       "7  Please answer with one of the option in the br...   \n",
       "8  Please answer with one of the option in the br...   \n",
       "9  Please answer with one of the option in the br...   \n",
       "\n",
       "                                              output  \n",
       "0  <analysis>\\n\\nThis is a clinical vignette desc...  \n",
       "1  <analysis>\\n\\nThis is a clinical vignette desc...  \n",
       "2  <analysis>\\n\\nThis is a question about a 27-ye...  \n",
       "3  <analysis>\\n\\nThis is a patient with signs and...  \n",
       "4  <analysis>\\n\\nThis is a patient with symptoms ...  \n",
       "5  <analysis>\\n\\nThis is a clinical vignette desc...  \n",
       "6  <analysis>\\n\\nThis is a clinical case question...  \n",
       "7  <analysis>\\n\\nThis is a question about determi...  \n",
       "8  <analysis>\\n\\nThis is a patient with a history...  \n",
       "9  <analysis>\\n\\nThis is a clinical vignette desc...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset[\"train\"].to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb65326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"\n",
    "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "539faa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for question, response in zip(inputs, outputs):\n",
    "        # Remove the \"Q:\" prefix from the question\n",
    "        question = question.replace(\"Q:\", \"\")\n",
    "        \n",
    "        # Append the EOS token to the response if it's not already there\n",
    "        if not response.endswith(tokenizer.eos_token):\n",
    "            response += tokenizer.eos_token\n",
    "            \n",
    "        text = train_prompt_style.format(question, response)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee12ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
      "### Question:\n",
      "A research group wants to assess the relationship between childhood diet and cardiovascular disease in adulthood. A prospective cohort study of 500 children between 10 to 15 years of age is conducted in which the participants' diets are recorded for 1 year and then the patients are assessed 20 years later for the presence of cardiovascular disease. A statistically significant association is found between childhood consumption of vegetables and decreased risk of hyperlipidemia and exercise tolerance. When these findings are submitted to a scientific journal, a peer reviewer comments that the researchers did not discuss the study's validity. Which of the following additional analyses would most likely address the concerns about this study's design?? \n",
      "{'A': 'Blinding', 'B': 'Crossover', 'C': 'Matching', 'D': 'Stratification', 'E': 'Randomization'},\n",
      "\n",
      "### Response:\n",
      "<analysis>\n",
      "\n",
      "This is a question about assessing the validity of a prospective cohort study. The study found an association between childhood diet and cardiovascular disease in adulthood. The peer reviewer is concerned that the researchers did not discuss the validity of the study design. \n",
      "\n",
      "To address concerns about validity in a prospective cohort study, we need to consider potential confounding factors that could influence the results. The additional analysis suggested should help control for confounding.\n",
      "</analysis>\n",
      "<answer>\n",
      "D: Stratification\n",
      "</answer><eos>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"mamachang/medical-reasoning\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "print(dataset[\"text\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da1dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt_style = \"\"\"\n",
    "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<analysis>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "405c9b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<analysis>\n",
      "\n",
      "\n",
      "### Answer:\n",
      "<answer>\n",
      "A. Blinding\n",
      "</answer>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = dataset[10]['input']\n",
    "question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "756eecef-67e6-4a0f-8edc-5ddd328fb55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nPlease answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\\n\\n### Question:\\nA research group wants to assess the relationship between childhood diet and cardiovascular disease in adulthood. A prospective cohort study of 500 children between 10 to 15 years of age is conducted in which the participants' diets are recorded for 1 year and then the patients are assessed 20 years later for the presence of cardiovascular disease. A statistically significant association is found between childhood consumption of vegetables and decreased risk of hyperlipidemia and exercise tolerance. When these findings are submitted to a scientific journal, a peer reviewer comments that the researchers did not discuss the study's validity. Which of the following additional analyses would most likely address the concerns about this study's design?? \\n{'A': 'Blinding', 'B': 'Crossover', 'C': 'Matching', 'D': 'Stratification', 'E': 'Randomization'},\\n\\n### Response:\\n<analysis>\\n<eos>\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inference_prompt_style.format(question) + tokenizer.eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                           # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
    "    r=64,                                    # Rank of the LoRA update matrices\n",
    "    bias=\"none\",                             # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"Magistral-Medical-Reasoning\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=0.2,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59128663",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b069148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<analysis>\n",
      "The question is asking about the validity concern raised in the peer reviewer's comment on the study. The study looked at the relationship between childhood diet and cardiovascular disease in adulthood. The peer reviewer says the researchers did not discuss the study's validity. \n",
      "\n",
      "To address the validity concern, we need to think of an additional analysis that would help assess whether the results are due to chance (selection bias) or an actual relationship between the two variables.\n",
      "\n",
      "Choice A, blinding, refers to hiding treatment groups and assessing outcome blindly, which is not relevant here. \n",
      "\n",
      "Choice B, crossover, involves matching participants so they serve as their own control, which also does not apply. \n",
      "\n",
      "Choice C, matching, involves balancing covariates between groups, which does not necessarily address validity concerns. \n",
      "\n",
      "Choice D, stratification, involves grouping participants based on their exposure levels and then analyzing within each stratum, which helps control for differences in outcome due to differences in exposure. This would be the best choice to address the validity concern here.\n",
      "\n",
      "Choice E, randomization, refers to assigning participants to groups at random, which is the principle of an experimental design and the foundation of validity. While randomization is important, it alone does not address validity concerns.\n",
      "</analysis>\n",
      "<answer>\n",
      "D: Stratification\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "<answer>\n",
      "D: Stratification\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer>\n",
      "</question>\n",
      "</analysis>\n",
      "</answer\n"
     ]
    }
   ],
   "source": [
    "question = dataset[10]['input']\n",
    "question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question,) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fbe03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2dcdab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c20d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d3a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd3d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec1419-ccad-4acf-9e79-22ac615b21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Quote: Imagination is more,\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device) # return_tensors='pt' means torch tensor, 'tf' means tensorflow tensor\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d0854-b425-4f72-89dc-cdd01d00aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Quote: Imagination is more\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76205389",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8, # rank\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19dcb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']['quote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09809802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"Quote: {example['quote'][0]}\\nAuthor: {example['author'][0]}\"\n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']['quote'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268274c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=5,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
